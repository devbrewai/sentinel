{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3925000",
   "metadata": {},
   "source": [
    "# Sanctions Screening Evaluation\n",
    "\n",
    "- **Purpose:** Evaluate sanctions screening accuracy and validate precision/recall targets\n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** November 18, 2025  \n",
    "- **Status:** In progress  \n",
    "- **License:** Apache 2.0\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the evaluation protocol for the sanctions screening module. The evaluation measures matching accuracy through a labeled test set and validates that the system meets production accuracy targets.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Precision@1: Percentage of queries where top candidate is the correct match (target: ≥95%)\n",
    "- Recall@top3: Percentage of queries where ground truth match appears in top 3 (target: ≥98%)\n",
    "- False Positive Rate: Percentage of non-matches incorrectly flagged as matches\n",
    "- Decision Accuracy: Alignment between predicted and expected decision categories\n",
    "\n",
    "The evaluation validates that the screening system correctly identifies sanctioned entities while minimizing false positives, meeting production readiness requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd7cc8",
   "metadata": {},
   "source": [
    "## Setup: Artifacts and Functions\n",
    "\n",
    "The evaluation loads artifacts generated by the implementation pipeline:\n",
    "\n",
    "- **Sanctions Index**: Canonicalized names and metadata (`sanctions_index.parquet`)\n",
    "- **Blocking Indices**: Inverted indices for candidate retrieval (`blocking_indices.json`)\n",
    "- **Metadata**: Version tracking and dataset statistics\n",
    "\n",
    "Helper functions for text normalization, tokenization, and screening are loaded to enable independent evaluation runs without re-executing the full implementation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ea27b",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent evaluation results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "716a07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n",
      " pandas: 2.3.3\n",
      " numpy: 2.3.3\n",
      " rapidfuzz: 3.14.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rapidfuzz as rf\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Environment configured successfully\")\n",
    "print(f\" pandas: {pd.__version__}\")\n",
    "print(f\" numpy: {np.__version__}\")\n",
    "print(f\" rapidfuzz: {rf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c53e45",
   "metadata": {},
   "source": [
    "### Load Artifacts\n",
    "\n",
    "The evaluation loads pre-computed artifacts from the implementation pipeline. The sanctions index contains 39,350 canonicalized name records with metadata. Blocking indices enable O(1) candidate retrieval through inverted index lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213466b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifacts...\n",
      "\n",
      "Loaded sanctions index: 39,350 records\n",
      "Loaded blocking indices:\n",
      " - First token index: 15,597 keys\n",
      " - Bucket index: 4 keys\n",
      " - Initials index: 15,986 keys\n",
      "\n",
      "Loaded metadata: version 2025-11-17T06:00:56.218723\n",
      "\n",
      "All artifacts loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "MODELS_DIR = PROJECT_ROOT / \"packages\" / \"models\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\" / \"processed\"\n",
    "\n",
    "\n",
    "print(\"Loading artifacts...\\n\")\n",
    "\n",
    "# Load sanctions index\n",
    "sanctions_index_path = MODELS_DIR / \"sanctions_index.parquet\"\n",
    "if not sanctions_index_path.exists():\n",
    "    raise FileNotFoundError(f\"Sanctions index not found: {sanctions_index_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "sanctions_index = pd.read_parquet(sanctions_index_path)\n",
    "print(f\"Loaded sanctions index: {len(sanctions_index):,} records\")\n",
    "\n",
    "# Load blocking indices\n",
    "blocking_indices_path = MODELS_DIR / \"blocking_indices.json\"\n",
    "if not blocking_indices_path.exists():\n",
    "    raise FileNotFoundError(f\"Blocking indices not found: {blocking_indices_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "with open(blocking_indices_path, 'r') as f:\n",
    "    blocking_indices = json.load(f)\n",
    "\n",
    "first_token_index = {k: v for k, v in blocking_indices['first_token'].items()}\n",
    "bucket_index = {k: v for k, v in blocking_indices['bucket'].items()}\n",
    "initials_index = {k: v for k, v in blocking_indices['initials'].items()}\n",
    "\n",
    "print(f\"Loaded blocking indices:\")\n",
    "print(f\" - First token index: {len(first_token_index):,} keys\")\n",
    "print(f\" - Bucket index: {len(bucket_index):,} keys\")\n",
    "print(f\" - Initials index: {len(initials_index):,} keys\")\n",
    "\n",
    "# Load metadata (optional, for version tracking)\n",
    "metadata_path = MODELS_DIR / \"sanctions_index_metadata.json\"\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        sanctions_index_metadata = json.load(f)\n",
    "    print(f\"\\nLoaded metadata: version {sanctions_index_metadata.get('created_at', 'unknown')}\")\n",
    "else:\n",
    "    sanctions_index_metadata = {}\n",
    "    print(\"[Warning] Metadata not found (optional)\")\n",
    "\n",
    "print(f\"\\nAll artifacts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad19a28",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Text normalization and tokenization functions are imported from the shared `packages.compliance.sanctions` module. This module provides standardized functions used by both `04_sanctions_screening.ipynb` and this evaluation notebook, ensuring consistency across the screening pipeline.\n",
    "\n",
    "The shared functions include:\n",
    "- `normalize_text()`: Text normalization for robust fuzzy matching\n",
    "- `tokenize()`: Tokenization with stopword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a7cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions imported successfully\n",
      "  - normalize_text: normalize_text\n",
      "  - tokenize: tokenize\n"
     ]
    }
   ],
   "source": [
    "from packages.compliance.sanctions import (\n",
    "    normalize_text,\n",
    "    tokenize\n",
    ")\n",
    "\n",
    "# Verify imports work\n",
    "print(\"Helper functions imported successfully\")\n",
    "print(f\"  - normalize_text: {normalize_text.__name__}\")\n",
    "print(f\"  - tokenize: {tokenize.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd5e0e",
   "metadata": {},
   "source": [
    "## Create Labeled Test Set\n",
    "\n",
    "To evaluate the screening system's accuracy, we need a labeled test set with known ground truth matches. This test set will include:\n",
    "\n",
    "- **Positive examples**: Query variations of names in the sanctions index (exact matches, normalized versions, case variations, typos)\n",
    "- **Negative examples**: Names that should NOT match any sanctions record (to test false positive rate)\n",
    "\n",
    "We'll sample diverse names from the sanctions index and create query variations to test different matching scenarios. This approach allows us to measure:\n",
    "- **Precision@1**: How often the top candidate is the correct match\n",
    "- **Recall@top3**: How often the ground truth appears in the top 3 results\n",
    "- **False Positive Rate**: How often non-matches are incorrectly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6561ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labeled test set...\n",
      "\n",
      "Created 250 test queries\n",
      " - With ground truth: 200\n",
      " - Non-matches: 50\n",
      "\n",
      "Variation type distribution:\n",
      " - exact: 50\n",
      " - normalized: 50\n",
      " - case: 50\n",
      " - typo: 50\n",
      " - non_match: 50\n",
      "\n",
      "Saved test set to: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/data_catalog/processed/sanctions_eval_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to introduce typos\n",
    "def _introduce_typo(name: str, n_typos: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Introduce minor typos for testing robustness.\n",
    "    \n",
    "    Randomly replaces or removes characters to simulate real-world\n",
    "    data entry errors.\n",
    "    \"\"\"\n",
    "    chars = list(name)\n",
    "    for _ in range(n_typos):\n",
    "        if len(chars) > 0:\n",
    "            idx = random.randint(0, len(chars) - 1)\n",
    "            # Replace with random character or remove\n",
    "            if random.random() < 0.5:\n",
    "                chars[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            else:\n",
    "                chars.pop(idx)\n",
    "    return ''.join(chars)\n",
    "\n",
    "# Function to create labeled test set\n",
    "def create_labeled_test_set(\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    n_samples: int = 80,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a labeled test set with ground truth matches.\n",
    "    \n",
    "    Samples names from sanctions index and creates query variations\n",
    "    with known ground truth matches.\n",
    "    \n",
    "    Args:\n",
    "        sanctions_index: DataFrame with sanctions records\n",
    "        n_samples: Number of base names to sample\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with test queries and ground truth labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    \n",
    "    # Sample diverse names from sanctions index\n",
    "    # Note: For a production system, you might want stratified sampling\n",
    "    # by country/program/entity_type, but for this case study we keep it simple\n",
    "    sampled = sanctions_index.sample(\n",
    "        n=min(n_samples, len(sanctions_index)),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Validate UIDs exist\n",
    "    valid_uids = set(sanctions_index['uid'].values)\n",
    "    \n",
    "    test_queries = []\n",
    "    \n",
    "    for _, row in sampled.iterrows():\n",
    "        original_name = row['name']\n",
    "        uid = row['uid']\n",
    "        \n",
    "        # Skip if UID is invalid\n",
    "        if uid not in valid_uids:\n",
    "            continue\n",
    "        \n",
    "        # Create query variations to test different matching scenarios\n",
    "        variations = [\n",
    "            # Exact match (should score very high)\n",
    "            {\n",
    "                'query': original_name,\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.95,\n",
    "                'variation_type': 'exact'\n",
    "            },\n",
    "            # Normalized version (tests normalization pipeline)\n",
    "            {\n",
    "                'query': row['name_norm'],\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'normalized'\n",
    "            },\n",
    "            # Case variation (tests case-insensitive matching)\n",
    "            {\n",
    "                'query': original_name.upper(),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'case'\n",
    "            },\n",
    "            # Minor typo (tests robustness to errors)\n",
    "            {\n",
    "                'query': _introduce_typo(original_name, n_typos=1),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.85,\n",
    "                'variation_type': 'typo'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add negative examples (non-matches) to test false positive rate\n",
    "        # Sample a random name that shouldn't match\n",
    "        non_match_candidates = sanctions_index[sanctions_index['uid'] != uid]\n",
    "        if len(non_match_candidates) > 0:\n",
    "            non_match_name = non_match_candidates.sample(1, random_state=random_state).iloc[0]['name']\n",
    "            \n",
    "            variations.append({\n",
    "                'query': non_match_name,\n",
    "                'ground_truth_uid': None,  # No match expected\n",
    "                'expected_score_min': None,\n",
    "                'variation_type': 'non_match'\n",
    "            })\n",
    "        \n",
    "        test_queries.extend(variations)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    test_df = pd.DataFrame(test_queries)\n",
    "    \n",
    "    # Validate all ground truth UIDs\n",
    "    if len(test_df) > 0:\n",
    "        invalid_uids = test_df[\n",
    "            test_df['ground_truth_uid'].notna() & \n",
    "            ~test_df['ground_truth_uid'].isin(valid_uids)\n",
    "        ]\n",
    "        if len(invalid_uids) > 0:\n",
    "            raise ValueError(f\"Found {len(invalid_uids)} invalid ground truth UIDs\")\n",
    "    \n",
    "    # Add metadata\n",
    "    test_df['query_id'] = range(len(test_df))\n",
    "    test_df['created_at'] = pd.Timestamp.now()\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Create test set\n",
    "print(\"Creating labeled test set...\")\n",
    "labeled_test_set = create_labeled_test_set(\n",
    "    sanctions_index,\n",
    "    n_samples=50,  # 50 names × ~4 variations = ~200 queries\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(labeled_test_set):,} test queries\")\n",
    "print(f\" - With ground truth: {labeled_test_set['ground_truth_uid'].notna().sum():,}\")\n",
    "print(f\" - Non-matches: {labeled_test_set['ground_truth_uid'].isna().sum():,}\")\n",
    "\n",
    "# Show distribution of variation types\n",
    "if 'variation_type' in labeled_test_set.columns:\n",
    "    print(f\"\\nVariation type distribution:\")\n",
    "    for var_type, count in labeled_test_set['variation_type'].value_counts().items():\n",
    "        print(f\" - {var_type}: {count:,}\")\n",
    "\n",
    "# Save test set for reproducibility\n",
    "test_set_path = DATA_DIR / \"sanctions_eval_labels.csv\"\n",
    "test_set_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "labeled_test_set.to_csv(test_set_path, index=False)\n",
    "print(f\"\\nSaved test set to: {test_set_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5b7a2",
   "metadata": {},
   "source": [
    "## Screening Function for Evaluation\n",
    "\n",
    "To evaluate the screening system, we need to implement the same screening logic used in the production pipeline. This ensures our evaluation results accurately reflect how the system will perform in production.\n",
    "\n",
    "**Implementation Approach:**\n",
    "\n",
    "For this case study, we keep the screening functions inline in the notebook rather than extracting them to a shared module. This makes the evaluation notebook self-contained and easier to follow, demonstrating the complete evaluation flow in one place.\n",
    "\n",
    "The screening pipeline consists of:\n",
    "1. **Blocking**: Retrieve candidate records using blocking indices (first token, token bucket, initials)\n",
    "2. **Scoring**: Compute similarity scores for each candidate using RapidFuzz\n",
    "3. **Ranking**: Sort candidates by composite score and return top-K results\n",
    "\n",
    "This matches the implementation in `04_sanctions_screening.ipynb` to ensure consistent evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad336985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing screening function...\n",
      "\n",
      "Query: 'BANCO NACIONAL DE CUBA'\n",
      "Found 3 candidates:\n",
      "  1. BANCO NACIONAL DE CUBA (score: 1.000, uid: SDN_306)\n",
      "  2. INSTITUTO NACIONAL DE TURISMO DE CUBA (score: 0.705, uid: SDN_1042)\n",
      "  3. BANCO INTERNACIONAL DE DESARROLLO, C.A. (score: 0.685, uid: SDN_25646)\n"
     ]
    }
   ],
   "source": [
    "# Blocking helper functions (matching implementation from sanctions screening)\n",
    "\n",
    "def get_first_token(tokens: List[str]) -> str:\n",
    "    \"\"\"Extract first token for prefix blocking.\"\"\"\n",
    "    return tokens[0] if tokens else \"\"\n",
    "\n",
    "def get_token_count_bucket(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Bucket names by token count for length-based blocking.\n",
    "    \n",
    "    Groups:\n",
    "    - \"tiny\": 0-1 tokens\n",
    "    - \"small\": 2 tokens  \n",
    "    - \"medium\": 3-4 tokens\n",
    "    - \"large\": 5+ tokens\n",
    "    \"\"\"\n",
    "    count = len(tokens)\n",
    "    if count <= 1:\n",
    "        return \"tiny\"\n",
    "    elif count == 2:\n",
    "        return \"small\"\n",
    "    elif count <= 4:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def get_initials_signature(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Create initials signature from first letter of each token.\n",
    "    \n",
    "    Examples:\n",
    "        ['john', 'doe'] → 'j-d'\n",
    "        ['al', 'qaida'] → 'a-q'\n",
    "        ['banco', 'nacional', 'cuba'] → 'b-n-c'\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    return \"-\".join(t[0] for t in tokens if t)\n",
    "\n",
    "def get_candidates_eval(\n",
    "    query_name: str,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    max_candidates: Optional[int] = None\n",
    ") -> Tuple[List[int], Dict[int, int]]:\n",
    "    \"\"\"\n",
    "    Get candidate indices using blocking keys with prioritization.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (candidate_list, priority_scores)\n",
    "        - candidate_list: Sorted list of candidate indices\n",
    "        - priority_scores: Dict mapping candidate index to priority (higher = appears in more strategies)\n",
    "    \"\"\"\n",
    "    # Normalize and tokenize query\n",
    "    query_norm = normalize_text(query_name)\n",
    "    query_tokens = tokenize(query_norm)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return [], {}\n",
    "    \n",
    "    # Extract blocking keys using helper functions\n",
    "    query_first = get_first_token(query_tokens)\n",
    "    query_bucket = get_token_count_bucket(query_tokens)\n",
    "    query_initials = get_initials_signature(query_tokens)\n",
    "    \n",
    "    # Collect candidates from each strategy separately\n",
    "    first_token_candidates = set()\n",
    "    bucket_candidates = set()\n",
    "    initials_candidates = set()\n",
    "    \n",
    "    # Strategy 1: First token match (most specific)\n",
    "    if query_first in first_token_index:\n",
    "        first_token_candidates.update(first_token_index[query_first])\n",
    "    \n",
    "    # Strategy 2: Token bucket match (less specific - can be huge)\n",
    "    if query_bucket in bucket_index:\n",
    "        bucket_candidates.update(bucket_index[query_bucket])\n",
    "    \n",
    "    # Strategy 3: Initials match (moderately specific)\n",
    "    if query_initials in initials_index:\n",
    "        initials_candidates.update(initials_index[query_initials])\n",
    "    \n",
    "    # Count how many strategies each candidate appears in (priority)\n",
    "    candidate_counts = {}\n",
    "    all_candidates = first_token_candidates | bucket_candidates | initials_candidates\n",
    "    \n",
    "    for idx in all_candidates:\n",
    "        count = 0\n",
    "        if idx in first_token_candidates:\n",
    "            count += 3  # First token is most specific - weight higher\n",
    "        if idx in initials_candidates:\n",
    "            count += 2  # Initials are moderately specific\n",
    "        if idx in bucket_candidates:\n",
    "            count += 1  # Bucket is least specific\n",
    "        candidate_counts[idx] = count\n",
    "    \n",
    "    # Sort by priority (higher priority first), then by index\n",
    "    candidate_list = sorted(all_candidates, key=lambda x: (-candidate_counts[x], x))\n",
    "    \n",
    "    # For very large candidate sets, prioritize high-priority candidates\n",
    "    # Candidates appearing in multiple strategies are more likely to be matches\n",
    "    if max_candidates is not None and len(candidate_list) > max_candidates:\n",
    "        # Take top priority candidates first\n",
    "        candidate_list = candidate_list[:max_candidates]\n",
    "    \n",
    "    return candidate_list, candidate_counts\n",
    "\n",
    "\n",
    "def compute_similarity_batch(\n",
    "    query_norm: str,\n",
    "    candidate_norm_list: List[str]\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute similarity scores for multiple candidates in batch.\n",
    "    \n",
    "    Uses rapidfuzz.process.cdist for vectorized scoring, which is much faster\n",
    "    than looping through candidates individually.\n",
    "\n",
    "    This function implements the same compute similarity batch strategy as in 04_sanctions_screening.ipynb\n",
    "    to ensure consistent evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        query_norm: Normalized query name (full string)\n",
    "        candidate_norm_list: List of candidate normalized strings\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of three numpy arrays (set, sort, partial scores) [0-100]\n",
    "    \"\"\"\n",
    "    # Batch compute token_set_ratio\n",
    "    set_scores = process.cdist(\n",
    "        [query_norm],\n",
    "        candidate_norm_list,\n",
    "        scorer=fuzz.token_set_ratio,\n",
    "        workers=4  # Parallel scoring on multi-core CPU\n",
    "    )[0]\n",
    "\n",
    "    # Batch compute token_sort_ratio\n",
    "    sort_scores = process.cdist(\n",
    "        [query_norm],\n",
    "        candidate_norm_list,\n",
    "        scorer=fuzz.token_sort_ratio,\n",
    "        workers=4\n",
    "    )[0]\n",
    "\n",
    "    # Batch compute partial_ratio\n",
    "    partial_scores = process.cdist(\n",
    "        [query_norm],\n",
    "        candidate_norm_list,\n",
    "        scorer=fuzz.partial_ratio,\n",
    "        workers=4\n",
    "    )[0]\n",
    "\n",
    "    return set_scores, sort_scores, partial_scores\n",
    "\n",
    "\n",
    "def composite_score_batch(\n",
    "    set_scores: np.ndarray,\n",
    "    sort_scores: np.ndarray,\n",
    "    partial_scores: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute composite scores for batch of candidates.\n",
    "    \n",
    "    Uses vectorized numpy operations for efficiency.\n",
    "\n",
    "    This function implements the same composite score batch strategy as in 04_sanctions_screening.ipynb\n",
    "    to ensure consistent evaluation results.\n",
    "    Returns:\n",
    "        Array of composite scores [0-1]\n",
    "    \"\"\"\n",
    "    # Weighted average: 0.45 * set + 0.35 * sort + 0.20 * partial\n",
    "    raw_scores = 0.45 * set_scores + 0.35 * sort_scores + 0.20 * partial_scores\n",
    "\n",
    "    # Rescale to [0, 1]\n",
    "    composite_scores = np.clip(raw_scores / 100.0, 0.0, 1.0)\n",
    "\n",
    "    return composite_scores\n",
    "\n",
    "def screen_query_eval(\n",
    "    query_name: str,\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    top_k: int = 3,\n",
    "    initial_candidates: int = 2000,  # Score this many first\n",
    "    expand_threshold: float = 0.85,  # If top score < this, expand\n",
    "    max_candidates: int = 3000  # Max to score if expanding\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Screen a query name and return top-K candidates with scores.\n",
    "    \n",
    "    Uses two-stage adaptive scoring:\n",
    "    1. Score top 2000 priority candidates\n",
    "    2. If top score is low (< 0.85), expand to 3000 candidates\n",
    "    This balances latency and recall.\n",
    "    \"\"\"\n",
    "    # Normalize and tokenize query\n",
    "    query_norm = normalize_text(query_name)\n",
    "    query_tokens = tokenize(query_norm)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Get candidates with prioritization\n",
    "    candidate_indices, priority_scores = get_candidates_eval(\n",
    "        query_name,\n",
    "        first_token_index,\n",
    "        bucket_index,\n",
    "        initials_index,\n",
    "        max_candidates=None\n",
    "    )\n",
    "    \n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "    \n",
    "    # Stage 1: Score top priority candidates\n",
    "    # Always include all high-priority candidates (priority >= 3)\n",
    "    high_priority_candidates = [\n",
    "        idx for idx in candidate_indices \n",
    "        if priority_scores.get(idx, 0) >= 3\n",
    "    ]\n",
    "    \n",
    "    # Build initial candidate set\n",
    "    if len(high_priority_candidates) > 0:\n",
    "        remaining_slots = initial_candidates - len(high_priority_candidates)\n",
    "        if remaining_slots > 0:\n",
    "            other_candidates = [\n",
    "                idx for idx in candidate_indices \n",
    "                if idx not in high_priority_candidates\n",
    "            ][:remaining_slots]\n",
    "            candidates_to_score = high_priority_candidates + other_candidates\n",
    "        else:\n",
    "            candidates_to_score = high_priority_candidates[:initial_candidates]\n",
    "    else:\n",
    "        candidates_to_score = candidate_indices[:initial_candidates]\n",
    "    \n",
    "    # Pre-extract candidate strings\n",
    "    candidate_norm_list = []\n",
    "    candidate_metadata = []\n",
    "    \n",
    "    for idx in candidates_to_score:\n",
    "        try:\n",
    "            candidate = sanctions_index.iloc[idx]\n",
    "            candidate_norm_list.append(candidate['name_norm'])\n",
    "            candidate_metadata.append({\n",
    "                'uid': candidate['uid'],\n",
    "                'name': candidate['name'],\n",
    "                'country': candidate.get('country'),\n",
    "                'program': candidate.get('program'),\n",
    "                'source': candidate.get('source')\n",
    "            })\n",
    "        except (IndexError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    if not candidate_norm_list:\n",
    "        return []\n",
    "    \n",
    "    # Stage 1: Batch score initial candidates\n",
    "    set_scores, sort_scores, partial_scores = compute_similarity_batch(\n",
    "        query_norm,\n",
    "        candidate_norm_list\n",
    "    )\n",
    "    \n",
    "    composite_scores = composite_score_batch(set_scores, sort_scores, partial_scores)\n",
    "    \n",
    "    # Check if we need to expand (two-stage approach)\n",
    "    top_score = float(np.max(composite_scores)) if len(composite_scores) > 0 else 0.0\n",
    "    \n",
    "    # Stage 2: If top score is low, expand candidate set\n",
    "    if top_score < expand_threshold and len(candidate_indices) > initial_candidates:\n",
    "        # Expand to include more candidates\n",
    "        additional_needed = max_candidates - len(candidates_to_score)\n",
    "        if additional_needed > 0:\n",
    "            additional_candidates = [\n",
    "                idx for idx in candidate_indices \n",
    "                if idx not in candidates_to_score\n",
    "            ][:additional_needed]\n",
    "            \n",
    "            # Add additional candidates\n",
    "            for idx in additional_candidates:\n",
    "                try:\n",
    "                    candidate = sanctions_index.iloc[idx]\n",
    "                    candidate_norm_list.append(candidate['name_norm'])\n",
    "                    candidate_metadata.append({\n",
    "                        'uid': candidate['uid'],\n",
    "                        'name': candidate['name'],\n",
    "                        'country': candidate.get('country'),\n",
    "                        'program': candidate.get('program'),\n",
    "                        'source': candidate.get('source')\n",
    "                    })\n",
    "                except (IndexError, KeyError):\n",
    "                    continue\n",
    "            \n",
    "            # Re-score expanded set\n",
    "            set_scores, sort_scores, partial_scores = compute_similarity_batch(\n",
    "                query_norm,\n",
    "                candidate_norm_list\n",
    "            )\n",
    "            composite_scores = composite_score_batch(set_scores, sort_scores, partial_scores)\n",
    "    \n",
    "    # Combine scores with metadata\n",
    "    scored_candidates = []\n",
    "    for i, metadata in enumerate(candidate_metadata):\n",
    "        scored_candidates.append({\n",
    "            **metadata,\n",
    "            'score': float(composite_scores[i]),\n",
    "            'sim_set': float(set_scores[i]) / 100.0,\n",
    "            'sim_sort': float(sort_scores[i]) / 100.0,\n",
    "            'sim_partial': float(partial_scores[i]) / 100.0\n",
    "        })\n",
    "    \n",
    "    # Sort by score (descending) and return top-K\n",
    "    scored_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return scored_candidates[:top_k]\n",
    "\n",
    "\n",
    "# Test the screening function\n",
    "print(\"Testing screening function...\")\n",
    "test_query = \"BANCO NACIONAL DE CUBA\"\n",
    "results = screen_query_eval(\n",
    "    test_query,\n",
    "    sanctions_index,\n",
    "    first_token_index,\n",
    "    bucket_index,\n",
    "    initials_index,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"Found {len(results)} candidates:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. {result['name']} (score: {result['score']:.3f}, uid: {result['uid']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd09d2",
   "metadata": {},
   "source": [
    "## Compute Evaluation Metrics\n",
    "\n",
    "Now that we have a labeled test set and screening functions, we can evaluate the system's performance. We'll run each query through the screening pipeline and compute key metrics:\n",
    "\n",
    "**Metrics to Compute:**\n",
    "- **Precision@1**: Percentage of queries where the top candidate is the correct match (target: ≥95%)\n",
    "- **Recall@top3**: Percentage of queries where the ground truth match appears in the top 3 results (target: ≥98%)\n",
    "- **False Positive Rate**: Percentage of non-matches incorrectly flagged as matches at different thresholds\n",
    "- **Latency Statistics**: p50, p95, p99 latencies to validate performance targets\n",
    "\n",
    "The evaluation function processes all queries, measures latency, and compares results against ground truth labels to compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae1a6290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Evaluating screening system...\n",
      "Processing 250 queries...\n",
      "\n",
      "  Processed 50 queries...\n",
      "  Processed 100 queries...\n",
      "  Processed 150 queries...\n",
      "  Processed 200 queries...\n",
      "  Processed 250 queries...\n",
      "\n",
      "Evaluation Results\n",
      "------------------------------------------------------------\n",
      "\n",
      "Precision@1:        97.5%\n",
      "Recall@top3:         98.0%\n",
      "FPR @ threshold 0.90: 100.0%\n",
      "FPR @ threshold 0.80: 100.0%\n",
      "\n",
      "Latency Statistics:\n",
      "  p50: 23.56 ms\n",
      "  p95: 49.63 ms\n",
      "  p99: 130.92 ms\n",
      "\n",
      "Total queries:      250\n",
      "  With ground truth: 200\n",
      "  Non-matches:       50\n",
      "\n",
      "Target Validation\n",
      "------------------------------------------------------------\n",
      "PASS - Precision@1 ≥ 95%\n",
      "PASS - Recall@top3 ≥ 98%\n",
      "PASS - Latency p95 < 50ms\n",
      "\n",
      "Overall: All targets met\n"
     ]
    }
   ],
   "source": [
    "## Compute evaluation metrics\n",
    "def evaluate_screening_system(\n",
    "    labeled_test_set: pd.DataFrame,\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    top_k: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate screening system on labeled test set.\n",
    "    \n",
    "    Processes each query in the test set, screens it against the sanctions index,\n",
    "    and compares results against ground truth labels to compute accuracy metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision@1, recall@top3, FPR, latency stats, and detailed results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    latencies = []\n",
    "    \n",
    "    print(\"Evaluating screening system...\")\n",
    "    print(f\"Processing {len(labeled_test_set):,} queries...\\n\")\n",
    "    \n",
    "    for idx, row in labeled_test_set.iterrows():\n",
    "        query = row['query']\n",
    "        ground_truth_uid = row['ground_truth_uid']\n",
    "        \n",
    "        # Screen query and measure latency\n",
    "        start_time = time.time()\n",
    "        candidates = screen_query_eval(\n",
    "            query,\n",
    "            sanctions_index,\n",
    "            first_token_index,\n",
    "            bucket_index,\n",
    "            initials_index,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        # Check if ground truth is in results\n",
    "        top1_match = candidates[0] if candidates else None\n",
    "        top1_correct = (\n",
    "            top1_match is not None and\n",
    "            top1_match['uid'] == ground_truth_uid\n",
    "        ) if pd.notna(ground_truth_uid) else None\n",
    "        \n",
    "        # Check if ground truth in top-K\n",
    "        topk_uids = [c['uid'] for c in candidates]\n",
    "        topk_match = (\n",
    "            ground_truth_uid in topk_uids\n",
    "        ) if pd.notna(ground_truth_uid) else None\n",
    "        \n",
    "        # Decision categories based on score thresholds\n",
    "        if top1_match:\n",
    "            score = top1_match['score']\n",
    "            if score >= 0.90:\n",
    "                decision = 'is_match'\n",
    "            elif score >= 0.80:\n",
    "                decision = 'review'\n",
    "            else:\n",
    "                decision = 'no_match'\n",
    "        else:\n",
    "            decision = 'no_match'\n",
    "            score = 0.0\n",
    "        \n",
    "        results.append({\n",
    "            'query_id': row['query_id'],\n",
    "            'query': query,\n",
    "            'ground_truth_uid': ground_truth_uid,\n",
    "            'top1_uid': top1_match['uid'] if top1_match else None,\n",
    "            'top1_score': score,\n",
    "            'top1_correct': top1_correct,\n",
    "            'topk_match': topk_match,\n",
    "            'decision': decision,\n",
    "            'latency_ms': latency_ms,\n",
    "            'num_candidates': len(candidates)\n",
    "        })\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1:,} queries...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Precision@1: Of queries with ground truth, how many have correct top-1?\n",
    "    queries_with_truth = results_df[results_df['ground_truth_uid'].notna()]\n",
    "    precision_at_1 = queries_with_truth['top1_correct'].mean() if len(queries_with_truth) > 0 else 0.0\n",
    "    \n",
    "    # Recall@top3: Of queries with ground truth, how many have match in top-3?\n",
    "    recall_at_top3 = queries_with_truth['topk_match'].mean() if len(queries_with_truth) > 0 else 0.0\n",
    "    \n",
    "    # False Positive Rate: Of non-matches, how many are flagged as matches?\n",
    "    non_matches = results_df[results_df['ground_truth_uid'].isna()]\n",
    "    fpr_at_90 = (\n",
    "        (non_matches['decision'] == 'is_match').mean()\n",
    "        if len(non_matches) > 0 else 0.0\n",
    "    )\n",
    "    fpr_at_80 = (\n",
    "        ((non_matches['decision'] == 'is_match') | \n",
    "         (non_matches['decision'] == 'review')).mean()\n",
    "        if len(non_matches) > 0 else 0.0\n",
    "    )\n",
    "    \n",
    "    # Latency statistics\n",
    "    latency_p50 = np.percentile(latencies, 50)\n",
    "    latency_p95 = np.percentile(latencies, 95)\n",
    "    latency_p99 = np.percentile(latencies, 99)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_at_1': precision_at_1,\n",
    "        'recall_at_top3': recall_at_top3,\n",
    "        'fpr_at_threshold_90': fpr_at_90,\n",
    "        'fpr_at_threshold_80': fpr_at_80,\n",
    "        'latency_p50_ms': latency_p50,\n",
    "        'latency_p95_ms': latency_p95,\n",
    "        'latency_p99_ms': latency_p99,\n",
    "        'num_queries': len(results_df),\n",
    "        'num_with_ground_truth': len(queries_with_truth),\n",
    "        'num_non_matches': len(non_matches)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'results': results_df,\n",
    "        'latencies': latencies\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = evaluate_screening_system(\n",
    "    labeled_test_set,\n",
    "    sanctions_index,\n",
    "    first_token_index,\n",
    "    bucket_index,\n",
    "    initials_index,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "metrics = evaluation_results['metrics']\n",
    "results_df = evaluation_results['results']\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nPrecision@1:        {metrics['precision_at_1']:.1%}\")\n",
    "print(f\"Recall@top3:         {metrics['recall_at_top3']:.1%}\")\n",
    "print(f\"FPR @ threshold 0.90: {metrics['fpr_at_threshold_90']:.1%}\")\n",
    "print(f\"FPR @ threshold 0.80: {metrics['fpr_at_threshold_80']:.1%}\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  p50: {metrics['latency_p50_ms']:.2f} ms\")\n",
    "print(f\"  p95: {metrics['latency_p95_ms']:.2f} ms\")\n",
    "print(f\"  p99: {metrics['latency_p99_ms']:.2f} ms\")\n",
    "print(f\"\\nTotal queries:      {metrics['num_queries']:,}\")\n",
    "print(f\"  With ground truth: {metrics['num_with_ground_truth']:,}\")\n",
    "print(f\"  Non-matches:       {metrics['num_non_matches']:,}\")\n",
    "\n",
    "# Check if targets are met\n",
    "print(\"\\nTarget Validation\")\n",
    "print(\"-\"*60)\n",
    "targets_met = {\n",
    "    'Precision@1 ≥ 95%': metrics['precision_at_1'] >= 0.95,\n",
    "    'Recall@top3 ≥ 98%': metrics['recall_at_top3'] >= 0.98,\n",
    "    'Latency p95 < 50ms': metrics['latency_p95_ms'] < 50.0\n",
    "}\n",
    "\n",
    "for target, met in targets_met.items():\n",
    "    status = \"PASS\" if met else \"FAIL\"\n",
    "    print(f\"{status} - {target}\")\n",
    "\n",
    "all_targets_met = all(targets_met.values())\n",
    "print(f\"\\nOverall: {'All targets met' if all_targets_met else 'Some targets not met'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908499d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (devbrew-payments-fraud-sanctions)",
   "language": "python",
   "name": "devbrew-payments-fraud-sanctions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
