{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3925000",
   "metadata": {},
   "source": [
    "# Sanctions Screening Evaluation\n",
    "\n",
    "- **Purpose:** Evaluate sanctions screening accuracy and validate precision/recall targets\n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** November 18, 2025  \n",
    "- **Status:** In progress  \n",
    "- **License:** Apache 2.0\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the evaluation protocol for the sanctions screening module. The evaluation measures matching accuracy through a labeled test set and validates that the system meets production accuracy targets.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Precision@1: Percentage of queries where top candidate is the correct match (target: ≥95%)\n",
    "- Recall@top3: Percentage of queries where ground truth match appears in top 3 (target: ≥98%)\n",
    "- False Positive Rate: Percentage of non-matches incorrectly flagged as matches\n",
    "- Decision Accuracy: Alignment between predicted and expected decision categories\n",
    "\n",
    "The evaluation validates that the screening system correctly identifies sanctioned entities while minimizing false positives, meeting production readiness requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd7cc8",
   "metadata": {},
   "source": [
    "## Setup: Artifacts and Functions\n",
    "\n",
    "The evaluation loads artifacts generated by the implementation pipeline:\n",
    "\n",
    "- **Sanctions Index**: Canonicalized names and metadata (`sanctions_index.parquet`)\n",
    "- **Blocking Indices**: Inverted indices for candidate retrieval (`blocking_indices.json`)\n",
    "- **Metadata**: Version tracking and dataset statistics\n",
    "\n",
    "Helper functions for text normalization, tokenization, and screening are loaded to enable independent evaluation runs without re-executing the full implementation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ea27b",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent evaluation results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n",
      " pandas: 2.3.3\n",
      " numpy: 2.3.3\n",
      " rapidfuzz: 3.14.1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rapidfuzz as rf\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Environment configured successfully\")\n",
    "print(f\" pandas: {pd.__version__}\")\n",
    "print(f\" numpy: {np.__version__}\")\n",
    "print(f\" rapidfuzz: {rf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c53e45",
   "metadata": {},
   "source": [
    "### Load Artifacts\n",
    "\n",
    "The evaluation loads pre-computed artifacts from the implementation pipeline. The sanctions index contains 39,350 canonicalized name records with metadata. Blocking indices enable O(1) candidate retrieval through inverted index lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213466b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifacts...\n",
      "\n",
      "Loaded sanctions index: 39,350 records\n",
      "Loaded blocking indices:\n",
      " - First token index: 15,597 keys\n",
      " - Bucket index: 4 keys\n",
      " - Initials index: 15,986 keys\n",
      "\n",
      "Loaded metadata: version 2025-11-17T06:00:56.218723\n",
      "\n",
      "All artifacts loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "MODELS_DIR = PROJECT_ROOT / \"packages\" / \"models\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\" / \"processed\"\n",
    "\n",
    "\n",
    "print(\"Loading artifacts...\\n\")\n",
    "\n",
    "# Load sanctions index\n",
    "sanctions_index_path = MODELS_DIR / \"sanctions_index.parquet\"\n",
    "if not sanctions_index_path.exists():\n",
    "    raise FileNotFoundError(f\"Sanctions index not found: {sanctions_index_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "sanctions_index = pd.read_parquet(sanctions_index_path)\n",
    "print(f\"Loaded sanctions index: {len(sanctions_index):,} records\")\n",
    "\n",
    "# Load blocking indices\n",
    "blocking_indices_path = MODELS_DIR / \"blocking_indices.json\"\n",
    "if not blocking_indices_path.exists():\n",
    "    raise FileNotFoundError(f\"Blocking indices not found: {blocking_indices_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "with open(blocking_indices_path, 'r') as f:\n",
    "    blocking_indices = json.load(f)\n",
    "\n",
    "first_token_index = {k: v for k, v in blocking_indices['first_token'].items()}\n",
    "bucket_index = {k: v for k, v in blocking_indices['bucket'].items()}\n",
    "initials_index = {k: v for k, v in blocking_indices['initials'].items()}\n",
    "\n",
    "print(f\"Loaded blocking indices:\")\n",
    "print(f\" - First token index: {len(first_token_index):,} keys\")\n",
    "print(f\" - Bucket index: {len(bucket_index):,} keys\")\n",
    "print(f\" - Initials index: {len(initials_index):,} keys\")\n",
    "\n",
    "# Load metadata (optional, for version tracking)\n",
    "metadata_path = MODELS_DIR / \"sanctions_index_metadata.json\"\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        sanctions_index_metadata = json.load(f)\n",
    "    print(f\"\\nLoaded metadata: version {sanctions_index_metadata.get('created_at', 'unknown')}\")\n",
    "else:\n",
    "    sanctions_index_metadata = {}\n",
    "    print(\"[Warning] Metadata not found (optional)\")\n",
    "\n",
    "print(f\"\\nAll artifacts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad19a28",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Text normalization and tokenization functions are imported from the shared `packages.compliance.sanctions` module. This module provides standardized functions used by both `04_sanctions_screening.ipynb` and this evaluation notebook, ensuring consistency across the screening pipeline.\n",
    "\n",
    "The shared functions include:\n",
    "- `normalize_text()`: Text normalization for robust fuzzy matching\n",
    "- `tokenize()`: Tokenization with stopword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions imported successfully\n",
      "  - normalize_text: normalize_text\n",
      "  - tokenize: tokenize\n"
     ]
    }
   ],
   "source": [
    "from packages.compliance.sanctions import (\n",
    "    normalize_text,\n",
    "    tokenize\n",
    ")\n",
    "\n",
    "# Verify imports work\n",
    "print(\"Helper functions imported successfully\")\n",
    "print(f\"  - normalize_text: {normalize_text.__name__}\")\n",
    "print(f\"  - tokenize: {tokenize.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd5e0e",
   "metadata": {},
   "source": [
    "## Create Labeled Test Set\n",
    "\n",
    "To evaluate the screening system's accuracy, we need a labeled test set with known ground truth matches. This test set will include:\n",
    "\n",
    "- **Positive examples**: Query variations of names in the sanctions index (exact matches, normalized versions, case variations, typos)\n",
    "- **Negative examples**: Names that should NOT match any sanctions record (to test false positive rate)\n",
    "\n",
    "We'll sample diverse names from the sanctions index and create query variations to test different matching scenarios. This approach allows us to measure:\n",
    "- **Precision@1**: How often the top candidate is the correct match\n",
    "- **Recall@top3**: How often the ground truth appears in the top 3 results\n",
    "- **False Positive Rate**: How often non-matches are incorrectly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6561ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labeled test set...\n",
      "\n",
      "Created 250 test queries\n",
      " - With ground truth: 200\n",
      " - Non-matches: 50\n",
      "\n",
      "Variation type distribution:\n",
      " - exact: 50\n",
      " - normalized: 50\n",
      " - case: 50\n",
      " - typo: 50\n",
      " - non_match: 50\n",
      "\n",
      "Saved test set to: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/data_catalog/processed/sanctions_eval_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to introduce typos\n",
    "def _introduce_typo(name: str, n_typos: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Introduce minor typos for testing robustness.\n",
    "    \n",
    "    Randomly replaces or removes characters to simulate real-world\n",
    "    data entry errors.\n",
    "    \"\"\"\n",
    "    chars = list(name)\n",
    "    for _ in range(n_typos):\n",
    "        if len(chars) > 0:\n",
    "            idx = random.randint(0, len(chars) - 1)\n",
    "            # Replace with random character or remove\n",
    "            if random.random() < 0.5:\n",
    "                chars[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            else:\n",
    "                chars.pop(idx)\n",
    "    return ''.join(chars)\n",
    "\n",
    "# Function to create labeled test set\n",
    "def create_labeled_test_set(\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    n_samples: int = 80,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a labeled test set with ground truth matches.\n",
    "    \n",
    "    Samples names from sanctions index and creates query variations\n",
    "    with known ground truth matches.\n",
    "    \n",
    "    Args:\n",
    "        sanctions_index: DataFrame with sanctions records\n",
    "        n_samples: Number of base names to sample\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with test queries and ground truth labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    \n",
    "    # Sample diverse names from sanctions index\n",
    "    # Note: For a production system, you might want stratified sampling\n",
    "    # by country/program/entity_type, but for this case study we keep it simple\n",
    "    sampled = sanctions_index.sample(\n",
    "        n=min(n_samples, len(sanctions_index)),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Validate UIDs exist\n",
    "    valid_uids = set(sanctions_index['uid'].values)\n",
    "    \n",
    "    test_queries = []\n",
    "    \n",
    "    for _, row in sampled.iterrows():\n",
    "        original_name = row['name']\n",
    "        uid = row['uid']\n",
    "        \n",
    "        # Skip if UID is invalid\n",
    "        if uid not in valid_uids:\n",
    "            continue\n",
    "        \n",
    "        # Create query variations to test different matching scenarios\n",
    "        variations = [\n",
    "            # Exact match (should score very high)\n",
    "            {\n",
    "                'query': original_name,\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.95,\n",
    "                'variation_type': 'exact'\n",
    "            },\n",
    "            # Normalized version (tests normalization pipeline)\n",
    "            {\n",
    "                'query': row['name_norm'],\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'normalized'\n",
    "            },\n",
    "            # Case variation (tests case-insensitive matching)\n",
    "            {\n",
    "                'query': original_name.upper(),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'case'\n",
    "            },\n",
    "            # Minor typo (tests robustness to errors)\n",
    "            {\n",
    "                'query': _introduce_typo(original_name, n_typos=1),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.85,\n",
    "                'variation_type': 'typo'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add negative examples (non-matches) to test false positive rate\n",
    "        # Sample a random name that shouldn't match\n",
    "        non_match_candidates = sanctions_index[sanctions_index['uid'] != uid]\n",
    "        if len(non_match_candidates) > 0:\n",
    "            non_match_name = non_match_candidates.sample(1, random_state=random_state).iloc[0]['name']\n",
    "            \n",
    "            variations.append({\n",
    "                'query': non_match_name,\n",
    "                'ground_truth_uid': None,  # No match expected\n",
    "                'expected_score_min': None,\n",
    "                'variation_type': 'non_match'\n",
    "            })\n",
    "        \n",
    "        test_queries.extend(variations)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    test_df = pd.DataFrame(test_queries)\n",
    "    \n",
    "    # Validate all ground truth UIDs\n",
    "    if len(test_df) > 0:\n",
    "        invalid_uids = test_df[\n",
    "            test_df['ground_truth_uid'].notna() & \n",
    "            ~test_df['ground_truth_uid'].isin(valid_uids)\n",
    "        ]\n",
    "        if len(invalid_uids) > 0:\n",
    "            raise ValueError(f\"Found {len(invalid_uids)} invalid ground truth UIDs\")\n",
    "    \n",
    "    # Add metadata\n",
    "    test_df['query_id'] = range(len(test_df))\n",
    "    test_df['created_at'] = pd.Timestamp.now()\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Create test set\n",
    "print(\"Creating labeled test set...\")\n",
    "labeled_test_set = create_labeled_test_set(\n",
    "    sanctions_index,\n",
    "    n_samples=50,  # 50 names × ~4 variations = ~200 queries\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(labeled_test_set):,} test queries\")\n",
    "print(f\" - With ground truth: {labeled_test_set['ground_truth_uid'].notna().sum():,}\")\n",
    "print(f\" - Non-matches: {labeled_test_set['ground_truth_uid'].isna().sum():,}\")\n",
    "\n",
    "# Show distribution of variation types\n",
    "if 'variation_type' in labeled_test_set.columns:\n",
    "    print(f\"\\nVariation type distribution:\")\n",
    "    for var_type, count in labeled_test_set['variation_type'].value_counts().items():\n",
    "        print(f\" - {var_type}: {count:,}\")\n",
    "\n",
    "# Save test set for reproducibility\n",
    "test_set_path = DATA_DIR / \"sanctions_eval_labels.csv\"\n",
    "test_set_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "labeled_test_set.to_csv(test_set_path, index=False)\n",
    "print(f\"\\nSaved test set to: {test_set_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (devbrew-payments-fraud-sanctions)",
   "language": "python",
   "name": "devbrew-payments-fraud-sanctions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
