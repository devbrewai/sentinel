{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3925000",
   "metadata": {},
   "source": [
    "# Sanctions Screening Evaluation\n",
    "\n",
    "- **Purpose:** Evaluate sanctions screening accuracy and validate precision/recall targets\n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** November 18, 2025  \n",
    "- **Status:** In progress  \n",
    "- **License:** Apache 2.0\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the evaluation protocol for the sanctions screening module. The evaluation measures matching accuracy through a labeled test set and validates that the system meets production accuracy targets.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Precision@1: Percentage of queries where top candidate is the correct match (target: ≥95%)\n",
    "- Recall@top3: Percentage of queries where ground truth match appears in top 3 (target: ≥98%)\n",
    "- False Positive Rate: Percentage of non-matches incorrectly flagged as matches\n",
    "- Decision Accuracy: Alignment between predicted and expected decision categories\n",
    "\n",
    "The evaluation validates that the screening system correctly identifies sanctioned entities while minimizing false positives, meeting production readiness requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd7cc8",
   "metadata": {},
   "source": [
    "## Setup: Artifacts and Functions\n",
    "\n",
    "The evaluation loads artifacts generated by the implementation pipeline:\n",
    "\n",
    "- **Sanctions Index**: Canonicalized names and metadata (`sanctions_index.parquet`)\n",
    "- **Blocking Indices**: Inverted indices for candidate retrieval (`blocking_indices.json`)\n",
    "- **Metadata**: Version tracking and dataset statistics\n",
    "\n",
    "Helper functions for text normalization, tokenization, and screening are loaded to enable independent evaluation runs without re-executing the full implementation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ea27b",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent evaluation results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "716a07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n",
      " pandas: 2.3.3\n",
      " numpy: 2.3.3\n",
      " rapidfuzz: 3.14.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rapidfuzz as rf\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Environment configured successfully\")\n",
    "print(f\" pandas: {pd.__version__}\")\n",
    "print(f\" numpy: {np.__version__}\")\n",
    "print(f\" rapidfuzz: {rf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c53e45",
   "metadata": {},
   "source": [
    "### Load Artifacts\n",
    "\n",
    "The evaluation loads pre-computed artifacts from the implementation pipeline. The sanctions index contains 39,350 canonicalized name records with metadata. Blocking indices enable O(1) candidate retrieval through inverted index lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213466b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifacts...\n",
      "\n",
      "Loaded sanctions index: 39,350 records\n",
      "Loaded blocking indices:\n",
      " - First token index: 15,597 keys\n",
      " - Bucket index: 4 keys\n",
      " - Initials index: 15,986 keys\n",
      "\n",
      "Loaded metadata: version 2025-11-17T06:00:56.218723\n",
      "\n",
      "All artifacts loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path configuration\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "MODELS_DIR = PROJECT_ROOT / \"packages\" / \"models\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\" / \"processed\"\n",
    "\n",
    "\n",
    "print(\"Loading artifacts...\\n\")\n",
    "\n",
    "# Load sanctions index\n",
    "sanctions_index_path = MODELS_DIR / \"sanctions_index.parquet\"\n",
    "if not sanctions_index_path.exists():\n",
    "    raise FileNotFoundError(f\"Sanctions index not found: {sanctions_index_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "sanctions_index = pd.read_parquet(sanctions_index_path)\n",
    "print(f\"Loaded sanctions index: {len(sanctions_index):,} records\")\n",
    "\n",
    "# Load blocking indices\n",
    "blocking_indices_path = MODELS_DIR / \"blocking_indices.json\"\n",
    "if not blocking_indices_path.exists():\n",
    "    raise FileNotFoundError(f\"Blocking indices not found: {blocking_indices_path}\\n\"\n",
    "                          f\"Please run notebooks/04_sanctions_screening.ipynb first to generate artifacts.\")\n",
    "\n",
    "with open(blocking_indices_path, 'r') as f:\n",
    "    blocking_indices = json.load(f)\n",
    "\n",
    "first_token_index = {k: v for k, v in blocking_indices['first_token'].items()}\n",
    "bucket_index = {k: v for k, v in blocking_indices['bucket'].items()}\n",
    "initials_index = {k: v for k, v in blocking_indices['initials'].items()}\n",
    "\n",
    "print(f\"Loaded blocking indices:\")\n",
    "print(f\" - First token index: {len(first_token_index):,} keys\")\n",
    "print(f\" - Bucket index: {len(bucket_index):,} keys\")\n",
    "print(f\" - Initials index: {len(initials_index):,} keys\")\n",
    "\n",
    "# Load metadata (optional, for version tracking)\n",
    "metadata_path = MODELS_DIR / \"sanctions_index_metadata.json\"\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        sanctions_index_metadata = json.load(f)\n",
    "    print(f\"\\nLoaded metadata: version {sanctions_index_metadata.get('created_at', 'unknown')}\")\n",
    "else:\n",
    "    sanctions_index_metadata = {}\n",
    "    print(\"[Warning] Metadata not found (optional)\")\n",
    "\n",
    "print(f\"\\nAll artifacts loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad19a28",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Text normalization and tokenization functions are imported from the shared `packages.compliance.sanctions` module. This module provides standardized functions used by both `04_sanctions_screening.ipynb` and this evaluation notebook, ensuring consistency across the screening pipeline.\n",
    "\n",
    "The shared functions include:\n",
    "- `normalize_text()`: Text normalization for robust fuzzy matching\n",
    "- `tokenize()`: Tokenization with stopword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a7cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions imported successfully\n",
      "  - normalize_text: normalize_text\n",
      "  - tokenize: tokenize\n"
     ]
    }
   ],
   "source": [
    "from packages.compliance.sanctions import (\n",
    "    normalize_text,\n",
    "    tokenize\n",
    ")\n",
    "\n",
    "# Verify imports work\n",
    "print(\"Helper functions imported successfully\")\n",
    "print(f\"  - normalize_text: {normalize_text.__name__}\")\n",
    "print(f\"  - tokenize: {tokenize.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd5e0e",
   "metadata": {},
   "source": [
    "## Create Labeled Test Set\n",
    "\n",
    "To evaluate the screening system's accuracy, we need a labeled test set with known ground truth matches. This test set will include:\n",
    "\n",
    "- **Positive examples**: Query variations of names in the sanctions index (exact matches, normalized versions, case variations, typos)\n",
    "- **Negative examples**: Names that should NOT match any sanctions record (to test false positive rate)\n",
    "\n",
    "We'll sample diverse names from the sanctions index and create query variations to test different matching scenarios. This approach allows us to measure:\n",
    "- **Precision@1**: How often the top candidate is the correct match\n",
    "- **Recall@top3**: How often the ground truth appears in the top 3 results\n",
    "- **False Positive Rate**: How often non-matches are incorrectly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6561ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labeled test set...\n",
      "\n",
      "Created 250 test queries\n",
      " - With ground truth: 200\n",
      " - Non-matches: 50\n",
      "\n",
      "Variation type distribution:\n",
      " - exact: 50\n",
      " - normalized: 50\n",
      " - case: 50\n",
      " - typo: 50\n",
      " - non_match: 50\n",
      "\n",
      "Saved test set to: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/data_catalog/processed/sanctions_eval_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to introduce typos\n",
    "def _introduce_typo(name: str, n_typos: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Introduce minor typos for testing robustness.\n",
    "    \n",
    "    Randomly replaces or removes characters to simulate real-world\n",
    "    data entry errors.\n",
    "    \"\"\"\n",
    "    chars = list(name)\n",
    "    for _ in range(n_typos):\n",
    "        if len(chars) > 0:\n",
    "            idx = random.randint(0, len(chars) - 1)\n",
    "            # Replace with random character or remove\n",
    "            if random.random() < 0.5:\n",
    "                chars[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            else:\n",
    "                chars.pop(idx)\n",
    "    return ''.join(chars)\n",
    "\n",
    "# Function to create labeled test set\n",
    "def create_labeled_test_set(\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    n_samples: int = 80,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a labeled test set with ground truth matches.\n",
    "    \n",
    "    Samples names from sanctions index and creates query variations\n",
    "    with known ground truth matches.\n",
    "    \n",
    "    Args:\n",
    "        sanctions_index: DataFrame with sanctions records\n",
    "        n_samples: Number of base names to sample\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with test queries and ground truth labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    \n",
    "    # Sample diverse names from sanctions index\n",
    "    # Note: For a production system, you might want stratified sampling\n",
    "    # by country/program/entity_type, but for this case study we keep it simple\n",
    "    sampled = sanctions_index.sample(\n",
    "        n=min(n_samples, len(sanctions_index)),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Validate UIDs exist\n",
    "    valid_uids = set(sanctions_index['uid'].values)\n",
    "    \n",
    "    test_queries = []\n",
    "    \n",
    "    for _, row in sampled.iterrows():\n",
    "        original_name = row['name']\n",
    "        uid = row['uid']\n",
    "        \n",
    "        # Skip if UID is invalid\n",
    "        if uid not in valid_uids:\n",
    "            continue\n",
    "        \n",
    "        # Create query variations to test different matching scenarios\n",
    "        variations = [\n",
    "            # Exact match (should score very high)\n",
    "            {\n",
    "                'query': original_name,\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.95,\n",
    "                'variation_type': 'exact'\n",
    "            },\n",
    "            # Normalized version (tests normalization pipeline)\n",
    "            {\n",
    "                'query': row['name_norm'],\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'normalized'\n",
    "            },\n",
    "            # Case variation (tests case-insensitive matching)\n",
    "            {\n",
    "                'query': original_name.upper(),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.90,\n",
    "                'variation_type': 'case'\n",
    "            },\n",
    "            # Minor typo (tests robustness to errors)\n",
    "            {\n",
    "                'query': _introduce_typo(original_name, n_typos=1),\n",
    "                'ground_truth_uid': uid,\n",
    "                'expected_score_min': 0.85,\n",
    "                'variation_type': 'typo'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add negative examples (non-matches) to test false positive rate\n",
    "        # Sample a random name that shouldn't match\n",
    "        non_match_candidates = sanctions_index[sanctions_index['uid'] != uid]\n",
    "        if len(non_match_candidates) > 0:\n",
    "            non_match_name = non_match_candidates.sample(1, random_state=random_state).iloc[0]['name']\n",
    "            \n",
    "            variations.append({\n",
    "                'query': non_match_name,\n",
    "                'ground_truth_uid': None,  # No match expected\n",
    "                'expected_score_min': None,\n",
    "                'variation_type': 'non_match'\n",
    "            })\n",
    "        \n",
    "        test_queries.extend(variations)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    test_df = pd.DataFrame(test_queries)\n",
    "    \n",
    "    # Validate all ground truth UIDs\n",
    "    if len(test_df) > 0:\n",
    "        invalid_uids = test_df[\n",
    "            test_df['ground_truth_uid'].notna() & \n",
    "            ~test_df['ground_truth_uid'].isin(valid_uids)\n",
    "        ]\n",
    "        if len(invalid_uids) > 0:\n",
    "            raise ValueError(f\"Found {len(invalid_uids)} invalid ground truth UIDs\")\n",
    "    \n",
    "    # Add metadata\n",
    "    test_df['query_id'] = range(len(test_df))\n",
    "    test_df['created_at'] = pd.Timestamp.now()\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Create test set\n",
    "print(\"Creating labeled test set...\")\n",
    "labeled_test_set = create_labeled_test_set(\n",
    "    sanctions_index,\n",
    "    n_samples=50,  # 50 names × ~4 variations = ~200 queries\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(labeled_test_set):,} test queries\")\n",
    "print(f\" - With ground truth: {labeled_test_set['ground_truth_uid'].notna().sum():,}\")\n",
    "print(f\" - Non-matches: {labeled_test_set['ground_truth_uid'].isna().sum():,}\")\n",
    "\n",
    "# Show distribution of variation types\n",
    "if 'variation_type' in labeled_test_set.columns:\n",
    "    print(f\"\\nVariation type distribution:\")\n",
    "    for var_type, count in labeled_test_set['variation_type'].value_counts().items():\n",
    "        print(f\" - {var_type}: {count:,}\")\n",
    "\n",
    "# Save test set for reproducibility\n",
    "test_set_path = DATA_DIR / \"sanctions_eval_labels.csv\"\n",
    "test_set_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "labeled_test_set.to_csv(test_set_path, index=False)\n",
    "print(f\"\\nSaved test set to: {test_set_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5b7a2",
   "metadata": {},
   "source": [
    "## Screening Function for Evaluation\n",
    "\n",
    "To evaluate the screening system, we need to implement the same screening logic used in the production pipeline. This ensures our evaluation results accurately reflect how the system will perform in production.\n",
    "\n",
    "**Implementation Approach:**\n",
    "\n",
    "For this case study, we keep the screening functions inline in the notebook rather than extracting them to a shared module. This makes the evaluation notebook self-contained and easier to follow, demonstrating the complete evaluation flow in one place.\n",
    "\n",
    "The screening pipeline consists of:\n",
    "1. **Blocking**: Retrieve candidate records using blocking indices (first token, token bucket, initials)\n",
    "2. **Scoring**: Compute similarity scores for each candidate using RapidFuzz\n",
    "3. **Ranking**: Sort candidates by composite score and return top-K results\n",
    "\n",
    "This matches the implementation in `04_sanctions_screening.ipynb` to ensure consistent evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad336985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing screening function...\n",
      "\n",
      "Query: 'BANCO NACIONAL DE CUBA'\n",
      "Found 3 candidates:\n",
      "  1. BANCO NACIONAL DE CUBA (score: 1.000, uid: SDN_306)\n",
      "  2. INSTITUTO NACIONAL DE TURISMO DE CUBA (score: 0.705, uid: SDN_1042)\n",
      "  3. ALBANIAN NATIONAL ARMY (score: 0.602, uid: SDN_7649)\n"
     ]
    }
   ],
   "source": [
    "# Blocking helper functions (matching implementation from sanctions screening)\n",
    "def get_first_token(tokens: List[str]) -> str:\n",
    "    \"\"\"Extract first token for prefix blocking.\"\"\"\n",
    "    return tokens[0] if tokens else \"\"\n",
    "\n",
    "def get_token_count_bucket(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Bucket names by token count for length-based blocking.\n",
    "    \n",
    "    Groups:\n",
    "    - \"tiny\": 0-1 tokens\n",
    "    - \"small\": 2 tokens  \n",
    "    - \"medium\": 3-4 tokens\n",
    "    - \"large\": 5+ tokens\n",
    "    \"\"\"\n",
    "    count = len(tokens)\n",
    "    if count <= 1:\n",
    "        return \"tiny\"\n",
    "    elif count == 2:\n",
    "        return \"small\"\n",
    "    elif count <= 4:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def get_initials_signature(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Create initials signature from first letter of each token.\n",
    "    \n",
    "    Examples:\n",
    "        ['john', 'doe'] → 'j-d'\n",
    "        ['al', 'qaida'] → 'a-q'\n",
    "        ['banco', 'nacional', 'cuba'] → 'b-n-c'\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    return \"-\".join(t[0] for t in tokens if t)\n",
    "\n",
    "def get_candidates_eval(\n",
    "    query_name: str,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    max_candidates: int = 500\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Get candidate indices using blocking keys.\n",
    "    \n",
    "    This function implements the same blocking strategy as in 04_sanctions_screening.ipynb\n",
    "    to ensure consistent evaluation results.\n",
    "    \"\"\"\n",
    "    # Normalize and tokenize query\n",
    "    query_norm = normalize_text(query_name)\n",
    "    query_tokens = tokenize(query_norm)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Extract blocking keys using helper functions\n",
    "    query_first = get_first_token(query_tokens)\n",
    "    query_bucket = get_token_count_bucket(query_tokens)\n",
    "    query_initials = get_initials_signature(query_tokens)\n",
    "    \n",
    "    # Collect candidate indices from all blocking strategies\n",
    "    candidate_indices = set()\n",
    "    \n",
    "    # Strategy 1: First token match\n",
    "    if query_first in first_token_index:\n",
    "        candidate_indices.update(first_token_index[query_first])\n",
    "    \n",
    "    # Strategy 2: Token bucket match (same complexity)\n",
    "    if query_bucket in bucket_index:\n",
    "        candidate_indices.update(bucket_index[query_bucket])\n",
    "    \n",
    "    # Strategy 3: Initials match\n",
    "    if query_initials in initials_index:\n",
    "        candidate_indices.update(initials_index[query_initials])\n",
    "    \n",
    "    # Limit candidates and return sorted list\n",
    "    candidate_list = sorted(list(candidate_indices))[:max_candidates]\n",
    "    \n",
    "    return candidate_list\n",
    "\n",
    "def compute_similarity_eval(\n",
    "    query_norm: str,\n",
    "    candidate_norm: str\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute similarity scores for a candidate.\n",
    "    \n",
    "    Uses RapidFuzz to compute three complementary similarity metrics:\n",
    "    - token_set_ratio: Set-based matching (order-independent)\n",
    "    - token_sort_ratio: Sorted token matching (order-independent)\n",
    "    - partial_ratio: Substring/partial matching\n",
    "    \"\"\"\n",
    "    # Compute similarities using RapidFuzz\n",
    "    similarities = {\n",
    "        'set': fuzz.token_set_ratio(query_norm, candidate_norm) / 100.0,\n",
    "        'sort': fuzz.token_sort_ratio(query_norm, candidate_norm) / 100.0,\n",
    "        'partial': fuzz.partial_ratio(query_norm, candidate_norm) / 100.0\n",
    "    }\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def composite_score_eval(similarities: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute weighted composite score from individual similarities.\n",
    "    \n",
    "    Uses weighted combination:\n",
    "    - 45% token_set_ratio (most important for name matching)\n",
    "    - 35% token_sort_ratio (handles word order variations)\n",
    "    - 20% partial_ratio (catches substring matches)\n",
    "    \n",
    "    Returns score in [0, 1] range.\n",
    "    \"\"\"\n",
    "    raw_score = (\n",
    "        0.45 * similarities['set'] +\n",
    "        0.35 * similarities['sort'] +\n",
    "        0.20 * similarities['partial']\n",
    "    )\n",
    "    return max(0.0, min(1.0, raw_score))\n",
    "\n",
    "def screen_query_eval(\n",
    "    query_name: str,\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    top_k: int = 3\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Screen a query name and return top-K candidates with scores.\n",
    "    \n",
    "    This is the main screening function that combines blocking, scoring, and ranking.\n",
    "    It implements the same logic as the production screening pipeline for consistent evaluation.\n",
    "    \"\"\"\n",
    "    # Normalize and tokenize query\n",
    "    query_norm = normalize_text(query_name)\n",
    "    query_tokens = tokenize(query_norm)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Get candidates using blocking\n",
    "    candidate_indices = get_candidates_eval(\n",
    "        query_name,\n",
    "        first_token_index,\n",
    "        bucket_index,\n",
    "        initials_index\n",
    "    )\n",
    "    \n",
    "    if not candidate_indices:\n",
    "        return []\n",
    "    \n",
    "    # Score all candidates\n",
    "    scored_candidates = []\n",
    "    for idx in candidate_indices:\n",
    "        candidate = sanctions_index.iloc[idx]\n",
    "        candidate_norm = candidate['name_norm']\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        similarities = compute_similarity_eval(query_norm, candidate_norm)\n",
    "        \n",
    "        # Compute composite score\n",
    "        score = composite_score_eval(similarities)\n",
    "        \n",
    "        # Store candidate with metadata\n",
    "        scored_candidates.append({\n",
    "            'uid': candidate['uid'],\n",
    "            'name': candidate['name'],\n",
    "            'score': score,\n",
    "            'sim_set': similarities['set'],\n",
    "            'sim_sort': similarities['sort'],\n",
    "            'sim_partial': similarities['partial'],\n",
    "            'country': candidate.get('country'),\n",
    "            'program': candidate.get('program'),\n",
    "            'source': candidate.get('source')\n",
    "        })\n",
    "    \n",
    "    # Sort by score (descending) and return top-K\n",
    "    scored_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return scored_candidates[:top_k]\n",
    "\n",
    "# Test the screening function\n",
    "print(\"Testing screening function...\")\n",
    "test_query = \"BANCO NACIONAL DE CUBA\"\n",
    "results = screen_query_eval(\n",
    "    test_query,\n",
    "    sanctions_index,\n",
    "    first_token_index,\n",
    "    bucket_index,\n",
    "    initials_index,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"Found {len(results)} candidates:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. {result['name']} (score: {result['score']:.3f}, uid: {result['uid']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd09d2",
   "metadata": {},
   "source": [
    "## Compute Evaluation Metrics\n",
    "\n",
    "Now that we have a labeled test set and screening functions, we can evaluate the system's performance. We'll run each query through the screening pipeline and compute key metrics:\n",
    "\n",
    "**Metrics to Compute:**\n",
    "- **Precision@1**: Percentage of queries where the top candidate is the correct match (target: ≥95%)\n",
    "- **Recall@top3**: Percentage of queries where the ground truth match appears in the top 3 results (target: ≥98%)\n",
    "- **False Positive Rate**: Percentage of non-matches incorrectly flagged as matches at different thresholds\n",
    "- **Latency Statistics**: p50, p95, p99 latencies to validate performance targets\n",
    "\n",
    "The evaluation function processes all queries, measures latency, and compares results against ground truth labels to compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a6290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Evaluating screening system...\n",
      "Processing 250 queries...\n",
      "\n",
      "  Processed 50 queries...\n",
      "  Processed 100 queries...\n",
      "  Processed 150 queries...\n",
      "  Processed 200 queries...\n",
      "  Processed 250 queries...\n",
      "\n",
      "Evaluation Results\n",
      "------------------------------------------------------------\n",
      "\n",
      "Precision@1:        2.0%\n",
      "Recall@top3:         2.0%\n",
      "FPR @ threshold 0.90: 0.0%\n",
      "FPR @ threshold 0.80: 0.0%\n",
      "\n",
      "Latency Statistics:\n",
      "  p50: 5.61 ms\n",
      "  p95: 6.45 ms\n",
      "  p99: 6.90 ms\n",
      "\n",
      "Total queries:      250\n",
      "  With ground truth: 200\n",
      "  Non-matches:       50\n",
      "\n",
      "Target Validation\n",
      "------------------------------------------------------------\n",
      "FAIL - Precision@1 ≥ 95%\n",
      "FAIL - Recall@top3 ≥ 98%\n",
      "PASS - Latency p95 < 50ms\n",
      "\n",
      "Overall: Some targets not met\n"
     ]
    }
   ],
   "source": [
    "## Compute evaluation metrics\n",
    "def evaluate_screening_system(\n",
    "    labeled_test_set: pd.DataFrame,\n",
    "    sanctions_index: pd.DataFrame,\n",
    "    first_token_index: Dict[str, List[int]],\n",
    "    bucket_index: Dict[str, List[int]],\n",
    "    initials_index: Dict[str, List[int]],\n",
    "    top_k: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate screening system on labeled test set.\n",
    "    \n",
    "    Processes each query in the test set, screens it against the sanctions index,\n",
    "    and compares results against ground truth labels to compute accuracy metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision@1, recall@top3, FPR, latency stats, and detailed results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    latencies = []\n",
    "    \n",
    "    print(\"Evaluating screening system...\")\n",
    "    print(f\"Processing {len(labeled_test_set):,} queries...\\n\")\n",
    "    \n",
    "    for idx, row in labeled_test_set.iterrows():\n",
    "        query = row['query']\n",
    "        ground_truth_uid = row['ground_truth_uid']\n",
    "        \n",
    "        # Screen query and measure latency\n",
    "        start_time = time.time()\n",
    "        candidates = screen_query_eval(\n",
    "            query,\n",
    "            sanctions_index,\n",
    "            first_token_index,\n",
    "            bucket_index,\n",
    "            initials_index,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        # Check if ground truth is in results\n",
    "        top1_match = candidates[0] if candidates else None\n",
    "        top1_correct = (\n",
    "            top1_match is not None and\n",
    "            top1_match['uid'] == ground_truth_uid\n",
    "        ) if pd.notna(ground_truth_uid) else None\n",
    "        \n",
    "        # Check if ground truth in top-K\n",
    "        topk_uids = [c['uid'] for c in candidates]\n",
    "        topk_match = (\n",
    "            ground_truth_uid in topk_uids\n",
    "        ) if pd.notna(ground_truth_uid) else None\n",
    "        \n",
    "        # Decision categories based on score thresholds\n",
    "        if top1_match:\n",
    "            score = top1_match['score']\n",
    "            if score >= 0.90:\n",
    "                decision = 'is_match'\n",
    "            elif score >= 0.80:\n",
    "                decision = 'review'\n",
    "            else:\n",
    "                decision = 'no_match'\n",
    "        else:\n",
    "            decision = 'no_match'\n",
    "            score = 0.0\n",
    "        \n",
    "        results.append({\n",
    "            'query_id': row['query_id'],\n",
    "            'query': query,\n",
    "            'ground_truth_uid': ground_truth_uid,\n",
    "            'top1_uid': top1_match['uid'] if top1_match else None,\n",
    "            'top1_score': score,\n",
    "            'top1_correct': top1_correct,\n",
    "            'topk_match': topk_match,\n",
    "            'decision': decision,\n",
    "            'latency_ms': latency_ms,\n",
    "            'num_candidates': len(candidates)\n",
    "        })\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1:,} queries...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Precision@1: Of queries with ground truth, how many have correct top-1?\n",
    "    queries_with_truth = results_df[results_df['ground_truth_uid'].notna()]\n",
    "    precision_at_1 = queries_with_truth['top1_correct'].mean() if len(queries_with_truth) > 0 else 0.0\n",
    "    \n",
    "    # Recall@top3: Of queries with ground truth, how many have match in top-3?\n",
    "    recall_at_top3 = queries_with_truth['topk_match'].mean() if len(queries_with_truth) > 0 else 0.0\n",
    "    \n",
    "    # False Positive Rate: Of non-matches, how many are flagged as matches?\n",
    "    non_matches = results_df[results_df['ground_truth_uid'].isna()]\n",
    "    fpr_at_90 = (\n",
    "        (non_matches['decision'] == 'is_match').mean()\n",
    "        if len(non_matches) > 0 else 0.0\n",
    "    )\n",
    "    fpr_at_80 = (\n",
    "        ((non_matches['decision'] == 'is_match') | \n",
    "         (non_matches['decision'] == 'review')).mean()\n",
    "        if len(non_matches) > 0 else 0.0\n",
    "    )\n",
    "    \n",
    "    # Latency statistics\n",
    "    latency_p50 = np.percentile(latencies, 50)\n",
    "    latency_p95 = np.percentile(latencies, 95)\n",
    "    latency_p99 = np.percentile(latencies, 99)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision_at_1': precision_at_1,\n",
    "        'recall_at_top3': recall_at_top3,\n",
    "        'fpr_at_threshold_90': fpr_at_90,\n",
    "        'fpr_at_threshold_80': fpr_at_80,\n",
    "        'latency_p50_ms': latency_p50,\n",
    "        'latency_p95_ms': latency_p95,\n",
    "        'latency_p99_ms': latency_p99,\n",
    "        'num_queries': len(results_df),\n",
    "        'num_with_ground_truth': len(queries_with_truth),\n",
    "        'num_non_matches': len(non_matches)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'results': results_df,\n",
    "        'latencies': latencies\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = evaluate_screening_system(\n",
    "    labeled_test_set,\n",
    "    sanctions_index,\n",
    "    first_token_index,\n",
    "    bucket_index,\n",
    "    initials_index,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "metrics = evaluation_results['metrics']\n",
    "results_df = evaluation_results['results']\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nPrecision@1:        {metrics['precision_at_1']:.1%}\")\n",
    "print(f\"Recall@top3:         {metrics['recall_at_top3']:.1%}\")\n",
    "print(f\"FPR @ threshold 0.90: {metrics['fpr_at_threshold_90']:.1%}\")\n",
    "print(f\"FPR @ threshold 0.80: {metrics['fpr_at_threshold_80']:.1%}\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  p50: {metrics['latency_p50_ms']:.2f} ms\")\n",
    "print(f\"  p95: {metrics['latency_p95_ms']:.2f} ms\")\n",
    "print(f\"  p99: {metrics['latency_p99_ms']:.2f} ms\")\n",
    "print(f\"\\nTotal queries:      {metrics['num_queries']:,}\")\n",
    "print(f\"  With ground truth: {metrics['num_with_ground_truth']:,}\")\n",
    "print(f\"  Non-matches:       {metrics['num_non_matches']:,}\")\n",
    "\n",
    "# Check if targets are met\n",
    "print(\"\\nTarget Validation\")\n",
    "print(\"-\"*60)\n",
    "targets_met = {\n",
    "    'Precision@1 ≥ 95%': metrics['precision_at_1'] >= 0.95,\n",
    "    'Recall@top3 ≥ 98%': metrics['recall_at_top3'] >= 0.98,\n",
    "    'Latency p95 < 50ms': metrics['latency_p95_ms'] < 50.0\n",
    "}\n",
    "\n",
    "for target, met in targets_met.items():\n",
    "    status = \"PASS\" if met else \"FAIL\"\n",
    "    print(f\"{status} - {target}\")\n",
    "\n",
    "all_targets_met = all(targets_met.values())\n",
    "print(f\"\\nOverall: {'All targets met' if all_targets_met else 'Some targets not met'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415da55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (devbrew-payments-fraud-sanctions)",
   "language": "python",
   "name": "devbrew-payments-fraud-sanctions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
